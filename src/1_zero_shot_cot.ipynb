{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23b2dd84-c58c-4467-ab7c-53627ae86d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 1: Zero-Shot Chain-of-Thought Baseline\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14b7c111-3928-4954-a8a9-1a5d90c41f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 1: Install Dependencies ---\n",
    "!pip install transformers>=4.35.0 datasets>=2.14.0 accelerate>=0.24.0 torch>=2.0.0 tqdm matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4760512-e376-4863-8392-a99d65745ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 2: Import Libraries ---\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98e6dc32-1fdb-4904-b930-7069050d253a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please log in to Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4db23eb49d4693b6e24ac3c36c9f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- CELL 3: Hugging Face Login ---\n",
    "print(\"Please log in to Hugging Face...\")\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ec912a2-2198-4a12-b0ce-4f57016bac63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ZERO-SHOT CHAIN-OF-THOUGHT BASELINE\n",
      "================================================================================\n",
      "Model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Test samples: Full dataset\n",
      "Output directory: ./small_project/zero_shot_cot\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 4: Configuration ---\n",
    "class Config:\n",
    "    # Model\n",
    "    MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "    \n",
    "    # Dataset\n",
    "    USE_SUBSET = False\n",
    "    TEST_SUBSET_SIZE = 50\n",
    "    \n",
    "    # Output\n",
    "    OUTPUT_DIR = \"./small_project/zero_shot_cot\"\n",
    "    \n",
    "    # Generation\n",
    "    GENERATION_MAX_NEW_TOKENS = 256\n",
    "    TEMPERATURE = 0.7\n",
    "    TOP_P = 0.9\n",
    "\n",
    "config = Config()\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"ZERO-SHOT CHAIN-OF-THOUGHT BASELINE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Model: {config.MODEL_NAME}\")\n",
    "print(f\"Test samples: {config.TEST_SUBSET_SIZE if config.USE_SUBSET else 'Full dataset'}\")\n",
    "print(f\"Output directory: {config.OUTPUT_DIR}\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7329f4a-ee74-48a4-a739-0b0d713b6754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 5: Helper Functions ---\n",
    "def extract_answer(text):\n",
    "    \"\"\"Extract numerical answer from text with multiple strategies\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Strategy 1: Find #### format\n",
    "    match = re.search(r'####\\s*(-?\\d+(?:,\\d+)*(?:\\.\\d+)?)', text)\n",
    "    if match:\n",
    "        return match.group(1).replace(',', '')\n",
    "    \n",
    "    # Strategy 2: Common patterns\n",
    "    patterns = [\n",
    "        r'answer is[:\\s]+(-?\\d+(?:,\\d+)*(?:\\.\\d+)?)',\n",
    "        r'=\\s*(-?\\d+(?:,\\d+)*(?:\\.\\d+)?)\\s*$',\n",
    "        r'total[:\\s]+(-?\\d+(?:,\\d+)*(?:\\.\\d+)?)',\n",
    "        r'result[:\\s]+(-?\\d+(?:,\\d+)*(?:\\.\\d+)?)',\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).replace(',', '')\n",
    "    \n",
    "    # Strategy 3: Last number in text\n",
    "    numbers = re.findall(r'-?\\d+(?:,\\d+)*(?:\\.\\d+)?', text)\n",
    "    if numbers:\n",
    "        return numbers[-1].replace(',', '')\n",
    "    \n",
    "    return None\n",
    "\n",
    "def create_inference_prompt(question):\n",
    "    \"\"\"Create zero-shot CoT prompt\"\"\"\n",
    "    return f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Solve this math problem step by step and provide the final answer after ####.\n",
    "\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def load_gsm8k_test():\n",
    "    \"\"\"Load GSM8K test set\"\"\"\n",
    "    print(\"Loading GSM8K test set...\")\n",
    "    dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "    \n",
    "    test_data = []\n",
    "    for item in tqdm(dataset[\"test\"], desc=\"Processing test data\"):\n",
    "        answer = extract_answer(item[\"answer\"])\n",
    "        if answer:\n",
    "            test_data.append({\n",
    "                \"question\": item[\"question\"],\n",
    "                \"answer\": answer,\n",
    "                \"full_solution\": item[\"answer\"]\n",
    "            })\n",
    "    \n",
    "    print(f\"Loaded {len(test_data)} test examples\")\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2792b109-4ec0-4f38-bee7-b30a84e00dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GSM8K test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ec1558b7df4c40be13abcb1dc140df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test data:   0%|          | 0/1319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1319 test examples\n",
      "\n",
      " Using 1319 test examples (full dataset)\n",
      " Test data saved to ./small_project/zero_shot_cot/test_data.json\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 6: Load Data ---\n",
    "full_test_data = load_gsm8k_test()\n",
    "\n",
    "if config.USE_SUBSET:\n",
    "    test_data = full_test_data[:config.TEST_SUBSET_SIZE]\n",
    "    print(f\"\\n Using {len(test_data)} test examples (subset mode)\")\n",
    "else:\n",
    "    test_data = full_test_data\n",
    "    print(f\"\\n Using {len(test_data)} test examples (full dataset)\")\n",
    "\n",
    "# Save test data\n",
    "with open(f\"{config.OUTPUT_DIR}/test_data.json\", \"w\") as f:\n",
    "    json.dump(test_data, f, indent=2)\n",
    "print(f\" Test data saved to {config.OUTPUT_DIR}/test_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "321492f0-bb7d-4ebb-bc78-558a994af3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871b5d65990b4e0a81f7315a0c8bfb9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: meta-llama/Llama-3.2-3B-Instruct\n",
      "Device: cuda:0\n",
      "dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 7: Load Model ---\n",
    "print(\"\\nLoading base model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Model loaded: {config.MODEL_NAME}\")\n",
    "print(f\"Device: {model.device}\")\n",
    "print(f\"dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "994d7c4e-3ab3-4e30-b062-44c7c3858e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 8: Generation Function ---\n",
    "def generate_answer(model, tokenizer, question):\n",
    "    \"\"\"Generate answer using zero-shot chain-of-thought\"\"\"\n",
    "    prompt = create_inference_prompt(question)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=config.GENERATION_MAX_NEW_TOKENS,\n",
    "            do_sample=True,\n",
    "            temperature=config.TEMPERATURE,\n",
    "            top_p=config.TOP_P,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1,\n",
    "        )\n",
    "    \n",
    "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the assistant's response\n",
    "    if \"<|start_header_id|>assistant<|end_header_id|>\" in full_text:\n",
    "        response = full_text.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1]\n",
    "        response = response.replace(\"<|eot_id|>\", \"\").strip()\n",
    "    else:\n",
    "        response = full_text[len(prompt):].strip()\n",
    "    \n",
    "    # Extract answer\n",
    "    predicted_answer = extract_answer(response)\n",
    "    \n",
    "    return response, predicted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23cdb547-e269-4cb2-9983-ba5877d21ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 9: Evaluation Function ---\n",
    "def evaluate_zero_shot(model, tokenizer, test_data):\n",
    "    \"\"\"Evaluate model with zero-shot chain-of-thought\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVALUATING ZERO-SHOT CHAIN-OF-THOUGHT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    results_log = []\n",
    "    error_analysis = {\n",
    "        \"no_answer_extracted\": 0,\n",
    "        \"wrong_answer\": 0,\n",
    "        \"correct\": 0,\n",
    "        \"repetitive_output\": 0,\n",
    "    }\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for idx, item in enumerate(tqdm(test_data, desc=\"Evaluating\")):\n",
    "        question = item[\"question\"]\n",
    "        correct_answer_str = item[\"answer\"]\n",
    "        \n",
    "        response, predicted_answer_str = generate_answer(model, tokenizer, question)\n",
    "        \n",
    "        # Check for repetitive output\n",
    "        words = response.split()\n",
    "        if len(words) > 10:\n",
    "            unique_ratio = len(set(words)) / len(words)\n",
    "            if unique_ratio < 0.3:\n",
    "                error_analysis[\"repetitive_output\"] += 1\n",
    "        \n",
    "        # Check correctness\n",
    "        is_correct = False\n",
    "        if predicted_answer_str is not None and correct_answer_str is not None:\n",
    "            try:\n",
    "                is_correct = abs(float(predicted_answer_str) - float(correct_answer_str)) < 0.01\n",
    "            except (ValueError, TypeError):\n",
    "                is_correct = predicted_answer_str.strip() == correct_answer_str.strip()\n",
    "        \n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "            error_analysis[\"correct\"] += 1\n",
    "        elif predicted_answer_str is None:\n",
    "            error_analysis[\"no_answer_extracted\"] += 1\n",
    "        else:\n",
    "            error_analysis[\"wrong_answer\"] += 1\n",
    "        \n",
    "        total += 1\n",
    "        \n",
    "        results_log.append({\n",
    "            \"index\": idx,\n",
    "            \"question\": question,\n",
    "            \"predicted_answer\": predicted_answer_str,\n",
    "            \"correct_answer\": correct_answer_str,\n",
    "            \"is_correct\": is_correct,\n",
    "            \"full_response\": response,\n",
    "            \"response_length\": len(response),\n",
    "            \"response_preview\": response[:200],\n",
    "        })\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EVALUATION RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} ({correct}/{total})\")\n",
    "    print(f\"\\nError Breakdown:\")\n",
    "    print(f\"  Correct: {error_analysis['correct']}\")\n",
    "    print(f\"  Wrong answer: {error_analysis['wrong_answer']}\")\n",
    "    print(f\"  No answer extracted: {error_analysis['no_answer_extracted']}\")\n",
    "    print(f\"  Repetitive output: {error_analysis['repetitive_output']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return accuracy, results_log, error_analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c7bbdaa-221e-4ddc-9a21-16ab1cb61c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start time: 2025-10-07 15:25:57\n",
      "\n",
      "================================================================================\n",
      "EVALUATING ZERO-SHOT CHAIN-OF-THOUGHT\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84995fd16eec4ede92d0ac84ea6b84fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION RESULTS\n",
      "================================================================================\n",
      "Accuracy: 0.5989 (790/1319)\n",
      "\n",
      "Error Breakdown:\n",
      "  Correct: 790\n",
      "  Wrong answer: 529\n",
      "  No answer extracted: 0\n",
      "  Repetitive output: 10\n",
      "================================================================================\n",
      "End time: 2025-10-07 17:17:43\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 10: Run Evaluation ---\n",
    "print(f\"\\nStart time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "accuracy, results_log, error_analysis = evaluate_zero_shot(model, tokenizer, test_data)\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba42ba0f-a441-4c46-b799-8cec795b777e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving results...\n",
      "✓ Results saved to:\n",
      "  - ./small_project/zero_shot_cot/results_summary.json\n",
      "  - ./small_project/zero_shot_cot/detailed_results.json\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 11: Save Results ---\n",
    "print(\"\\nSaving results...\")\n",
    "\n",
    "# Main results summary\n",
    "results_summary = {\n",
    "    \"experiment\": \"zero_shot_cot\",\n",
    "    \"model\": config.MODEL_NAME,\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"config\": {\n",
    "        \"test_size\": len(test_data),\n",
    "        \"max_new_tokens\": config.GENERATION_MAX_NEW_TOKENS,\n",
    "        \"temperature\": config.TEMPERATURE,\n",
    "        \"top_p\": config.TOP_P,\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"correct\": int(accuracy * len(test_data)),\n",
    "        \"total\": len(test_data),\n",
    "    },\n",
    "    \"error_analysis\": error_analysis,\n",
    "}\n",
    "\n",
    "with open(f\"{config.OUTPUT_DIR}/results_summary.json\", \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "# Detailed results with all predictions\n",
    "with open(f\"{config.OUTPUT_DIR}/detailed_results.json\", \"w\") as f:\n",
    "    json.dump(results_log, f, indent=2)\n",
    "\n",
    "print(f\"✓ Results saved to:\")\n",
    "print(f\"  - {config.OUTPUT_DIR}/results_summary.json\")\n",
    "print(f\"  - {config.OUTPUT_DIR}/detailed_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b7827b1-ad5d-4da1-bd9e-a5268907beb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAMPLE PREDICTIONS\n",
      "================================================================================\n",
      "\n",
      "CORRECT PREDICTIONS (showing up to 3):\n",
      "\n",
      "1. Question: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for ...\n",
      "   Predicted: 18\n",
      "   Correct: 18\n",
      "   Response: umber of eggs laid per day that are not eaten or used.\n",
      "Janet lays 16 eggs per day. She eats 3 eggs for breakfast, so:\n",
      "16 - 3 = 13 eggs left\n",
      "\n",
      "Step 2: Subtract the eggs used to bake muffins.\n",
      "She uses 4 ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. Question: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it ...\n",
      "   Predicted: 3\n",
      "   Correct: 3\n",
      "   Response: 2 bolts\n",
      "2. Calculate how much white fiber is needed: Half of 2 bolts is 1 bolt\n",
      "3. Add the amount of blue fiber and white fiber together: 2 + 1 = 3\n",
      "\n",
      "The final answer is: ###...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. Question: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repai...\n",
      "   Predicted: 70000\n",
      "   Correct: 70000\n",
      "   Response: pairs, so the total cost is: $80,000 + $50,000 = $130,000\n",
      "3. The repairs increased the value of the house by 150%, which means the new value of the house is 250% of its original price (100% + 150% inc...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "INCORRECT PREDICTIONS (showing up to 3):\n",
      "\n",
      "1. Question: Every day, Wendi feeds each of her chickens three cups of mixed chicken feed, containing seeds, meal...\n",
      "   Predicted: 140\n",
      "   Correct: 20\n",
      "   Response: the chickens a total of 15 + 25 = 40 cups of feed.\n",
      "\n",
      "## Step 2: Determine the number of chickens Wendi has.\n",
      "Wendi has 20 chickens in her flock.\n",
      "\n",
      "## Step 3: Calculate how much feed one chicken gets per ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. Question: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second g...\n",
      "   Predicted: 16\n",
      "   Correct: 64\n",
      "   Response: of the original price, which is 0.6 x $5 = $3.\n",
      "3. Since Kylar wants to buy 16 glasses, we need to find out how many glasses are not the first one: 16 - 1 = 15 glasses.\n",
      "4. The cost of these 15 glasses ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. Question: Carla is downloading a 200 GB file. Normally she can download 2 GB/minute, but 40% of the way throug...\n",
      "   Predicted: 4\n",
      "   Correct: 160\n",
      "   Response: a downloads at a rate of 2 GB/minute, we need to calculate how long it takes to download 160 GB (80% of 200 GB).\n",
      "\n",
      "Time = Total size / Download speed\n",
      "= 160 GB / 2 GB/minute\n",
      "= 80 minutes\n",
      "\n",
      "**Step 2: Add ...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 12: Sample Predictions ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show correct predictions\n",
    "correct_preds = [r for r in results_log if r['is_correct']]\n",
    "if correct_preds:\n",
    "    print(f\"\\nCORRECT PREDICTIONS (showing up to 3):\")\n",
    "    for i, r in enumerate(correct_preds[:3]):\n",
    "        print(f\"\\n{i+1}. Question: {r['question'][:100]}...\")\n",
    "        print(f\"   Predicted: {r['predicted_answer']}\")\n",
    "        print(f\"   Correct: {r['correct_answer']}\")\n",
    "        print(f\"   Response: {r['response_preview']}...\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Show errors\n",
    "error_preds = [r for r in results_log if not r['is_correct']]\n",
    "if error_preds:\n",
    "    print(f\"\\nINCORRECT PREDICTIONS (showing up to 3):\")\n",
    "    for i, r in enumerate(error_preds[:3]):\n",
    "        print(f\"\\n{i+1}. Question: {r['question'][:100]}...\")\n",
    "        print(f\"   Predicted: {r['predicted_answer']}\")\n",
    "        print(f\"   Correct: {r['correct_answer']}\")\n",
    "        print(f\"   Response: {r['response_preview']}...\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf39d84b-13a8-4396-8f4e-1d7cdcd6dd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DETAILED ERROR ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Total errors: 529\n",
      "Errors with no predicted answer: 0\n",
      "Errors with wrong answer: 529\n",
      "Errors with repetitive output: 5\n",
      "\n",
      "Response length statistics:\n",
      "  Mean: 586.7 chars\n",
      "  Min: 122 chars\n",
      "  Max: 1203 chars\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 13: Detailed Error Analysis ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED ERROR ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "errors = [r for r in results_log if not r['is_correct']]\n",
    "print(f\"\\nTotal errors: {len(errors)}\")\n",
    "print(f\"Errors with no predicted answer: {sum(1 for e in errors if e['predicted_answer'] is None)}\")\n",
    "print(f\"Errors with wrong answer: {sum(1 for e in errors if e['predicted_answer'] is not None)}\")\n",
    "\n",
    "# Check for repetitive outputs\n",
    "repetitive_errors = [e for e in errors if e['response_length'] > 0 and len(set(e['full_response'].split())) / len(e['full_response'].split()) < 0.3]\n",
    "print(f\"Errors with repetitive output: {len(repetitive_errors)}\")\n",
    "\n",
    "# Analyze response lengths\n",
    "if results_log:\n",
    "    response_lengths = [r['response_length'] for r in results_log]\n",
    "    print(f\"\\nResponse length statistics:\")\n",
    "    print(f\"  Mean: {sum(response_lengths)/len(response_lengths):.1f} chars\")\n",
    "    print(f\"  Min: {min(response_lengths)} chars\")\n",
    "    print(f\"  Max: {max(response_lengths)} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d750c7-1457-4f2c-863a-0626986df65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 14: Visualizations ---\n",
    "print(\"\\nGenerating visualizations...\")\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Accuracy pie chart\n",
    "ax1.pie([error_analysis['correct'], len(test_data) - error_analysis['correct']], \n",
    "        labels=['Correct', 'Incorrect'], \n",
    "        autopct='%1.1f%%',\n",
    "        colors=['#2ecc71', '#e74c3c'],\n",
    "        startangle=90)\n",
    "ax1.set_title(f'Overall Accuracy: {accuracy:.2%}', fontweight='bold')\n",
    "\n",
    "# 2. Error breakdown\n",
    "error_types = ['Wrong\\nAnswer', 'No Answer\\nExtracted', 'Repetitive\\nOutput']\n",
    "error_counts = [\n",
    "    error_analysis['wrong_answer'], \n",
    "    error_analysis['no_answer_extracted'], \n",
    "    error_analysis['repetitive_output']\n",
    "]\n",
    "ax2.bar(error_types, error_counts, color=['#e74c3c', '#f39c12', '#9b59b6'])\n",
    "ax2.set_ylabel('Count', fontweight='bold')\n",
    "ax2.set_title('Error Type Breakdown', fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Response length distribution\n",
    "response_lengths = [r['response_length'] for r in results_log]\n",
    "ax3.hist(response_lengths, bins=20, color='#3498db', alpha=0.7, edgecolor='black')\n",
    "ax3.set_xlabel('Response Length (characters)', fontweight='bold')\n",
    "ax3.set_ylabel('Frequency', fontweight='bold')\n",
    "ax3.set_title('Response Length Distribution', fontweight='bold')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Correct vs Incorrect response lengths\n",
    "correct_lengths = [r['response_length'] for r in results_log if r['is_correct']]\n",
    "incorrect_lengths = [r['response_length'] for r in results_log if not r['is_correct']]\n",
    "\n",
    "if correct_lengths and incorrect_lengths:\n",
    "    ax4.boxplot([correct_lengths, incorrect_lengths], labels=['Correct', 'Incorrect'])\n",
    "    ax4.set_ylabel('Response Length (characters)', fontweight='bold')\n",
    "    ax4.set_title('Response Length: Correct vs Incorrect', fontweight='bold')\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'Insufficient data', ha='center', va='center', transform=ax4.transAxes)\n",
    "    ax4.set_title('Response Length Comparison', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{config.OUTPUT_DIR}/evaluation_results.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Visualization saved to {config.OUTPUT_DIR}/evaluation_results.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e63f6d3-53d7-4d63-8644-d3ea50121a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 15: Sample Responses Analysis ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE FULL RESPONSES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show one complete correct response\n",
    "if correct_preds:\n",
    "    print(\"\\n[CORRECT EXAMPLE]\")\n",
    "    sample = correct_preds[0]\n",
    "    print(f\"Question: {sample['question']}\")\n",
    "    print(f\"\\nModel Response:\")\n",
    "    print(sample['full_response'])\n",
    "    print(f\"\\nExtracted Answer: {sample['predicted_answer']}\")\n",
    "    print(f\"Correct Answer: {sample['correct_answer']}\")\n",
    "\n",
    "# Show one complete incorrect response\n",
    "if error_preds:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"\\n[INCORRECT EXAMPLE]\")\n",
    "    sample = error_preds[0]\n",
    "    print(f\"Question: {sample['question']}\")\n",
    "    print(f\"\\nModel Response:\")\n",
    "    print(sample['full_response'])\n",
    "    print(f\"\\nExtracted Answer: {sample['predicted_answer']}\")\n",
    "    print(f\"Correct Answer: {sample['correct_answer']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc49ef70-adb5-4346-928c-55d720439539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 16: Generate Report ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "report = f\"\"\"\n",
    "ZERO-SHOT CHAIN-OF-THOUGHT EVALUATION REPORT\n",
    "{'='*80}\n",
    "\n",
    "Model: {config.MODEL_NAME}\n",
    "Test Set Size: {len(test_data)}\n",
    "Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "OVERALL RESULTS\n",
    "{'='*80}\n",
    "Accuracy: {accuracy:.4f} ({int(accuracy * len(test_data))}/{len(test_data)})\n",
    "\n",
    "ERROR ANALYSIS\n",
    "{'='*80}\n",
    "Total Errors: {len(errors)}\n",
    "  - Wrong Answer: {error_analysis['wrong_answer']} ({error_analysis['wrong_answer']/len(test_data)*100:.1f}%)\n",
    "  - No Answer Extracted: {error_analysis['no_answer_extracted']} ({error_analysis['no_answer_extracted']/len(test_data)*100:.1f}%)\n",
    "  - Repetitive Output: {error_analysis['repetitive_output']} ({error_analysis['repetitive_output']/len(test_data)*100:.1f}%)\n",
    "\n",
    "RESPONSE STATISTICS\n",
    "{'='*80}\n",
    "Average Response Length: {sum(response_lengths)/len(response_lengths):.1f} characters\n",
    "Min Response Length: {min(response_lengths)} characters\n",
    "Max Response Length: {max(response_lengths)} characters\n",
    "\n",
    "NOTES\n",
    "{'='*80}\n",
    "- This is a zero-shot evaluation (no fine-tuning)\n",
    "- The model uses chain-of-thought prompting\n",
    "- Temperature: {config.TEMPERATURE}, Top-p: {config.TOP_P}\n",
    "- Generation uses sampling (not greedy decoding)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{config.OUTPUT_DIR}/evaluation_report.txt\", \"w\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(report)\n",
    "print(f\"Report saved to {config.OUTPUT_DIR}/evaluation_report.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c2f21-0f82-485c-a1a8-0ed05889c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 17: Final Summary ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ZERO-SHOT CoT EVALUATION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nAll results saved to: {config.OUTPUT_DIR}/\")\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  1. test_data.json - Test dataset\")\n",
    "print(\"  2. results_summary.json - Summary statistics\")\n",
    "print(\"  3. detailed_results.json - All predictions\")\n",
    "print(\"  4. evaluation_results.png - Visualizations\")\n",
    "print(\"  5. evaluation_report.txt - Text report\")\n",
    "print(\"\\nKey Findings:\")\n",
    "print(f\"  - Baseline accuracy: {accuracy:.4f}\")\n",
    "print(f\"  - Total predictions: {len(test_data)}\")\n",
    "print(f\"  - Correct predictions: {int(accuracy * len(test_data))}\")\n",
    "print(f\"  - Main error type: {'No answer extracted' if error_analysis['no_answer_extracted'] > error_analysis['wrong_answer'] else 'Wrong answer'}\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
